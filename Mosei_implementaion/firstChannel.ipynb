{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text features shape: torch.Size([2089, 1, 768])\n",
      "Acoustic features shape: torch.Size([2089, 16])\n",
      "torch.Size([2089, 1, 768])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Define the CrossModalAttention model\n",
    "class CrossModalAttention(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(CrossModalAttention, self).__init__()\n",
    "        self.text_linear = nn.Linear(input_dim, output_dim)\n",
    "        self.audio_linear = nn.Linear(16, output_dim)  # Matching acoustic features with text output_dim\n",
    "        self.query_linear = nn.Linear(output_dim, output_dim)\n",
    "        self.key_linear = nn.Linear(output_dim, output_dim)\n",
    "        self.value_linear = nn.Linear(output_dim, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def forward(self, text_features, acoustic_features):\n",
    "        # Reshape acoustic features to match text features dimension\n",
    "        acoustic_features = acoustic_features.unsqueeze(1).expand(-1, text_features.size(1), -1)\n",
    "        \n",
    "        # Step 1: Embedding\n",
    "        ft = self.text_linear(text_features)  # [batch_size, seq_len, output_dim]\n",
    "        fa = self.audio_linear(acoustic_features)  # [batch_size, seq_len, output_dim]\n",
    "        \n",
    "        # Step 2: Linear transformations for Query, Key, Value\n",
    "        Q_t = self.query_linear(ft)  # [batch_size, seq_len, output_dim]\n",
    "        K_a = self.key_linear(fa)    # [batch_size, seq_len, output_dim]\n",
    "        V_a = self.value_linear(fa)  # [batch_size, seq_len, output_dim]\n",
    "\n",
    "        # Step 3: Cross-modal Attention\n",
    "        attention_scores = self.softmax(torch.matmul(Q_t, K_a.transpose(-2, -1)))  # [batch_size, seq_len, seq_len]\n",
    "        attention_output = torch.matmul(attention_scores, V_a)  # [batch_size, seq_len, output_dim]\n",
    "        \n",
    "        # Step 4: Residual connection\n",
    "        F1 = ft + attention_output  # [batch_size, seq_len, output_dim]\n",
    "        \n",
    "        return F1\n",
    "\n",
    "# Function to load features from .npy files in a directory\n",
    "def load_features(feature_dir):\n",
    "    feature_files = sorted(os.listdir(feature_dir))\n",
    "    features = []\n",
    "    for file in feature_files:\n",
    "        if file.endswith('.npy'):  # Load only .npy files\n",
    "            file_path = os.path.join(feature_dir, file)\n",
    "            try:\n",
    "                data = np.load(file_path, allow_pickle=True)\n",
    "                features.append(data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file_path}: {e}\")\n",
    "    features = np.stack(features, axis=0)  # Stack into a single tensor\n",
    "    return torch.tensor(features, dtype=torch.float32)\n",
    "\n",
    "# Directories containing the .npy files for each modality\n",
    "text_feature_dir = r\"/Users/dinesh/College/final proj/attempt3/features/text\"\n",
    "audio_feature_dir = r\"/Users/dinesh/College/final proj/attempt3/features/audio\"\n",
    "\n",
    "# Load features\n",
    "text_features = load_features(text_feature_dir)  # Shape: [batch_size, seq_len, input_dim]\n",
    "acoustic_features = load_features(audio_feature_dir)  # Shape: [batch_size, seq_len, input_dim]\n",
    "\n",
    "# Ensure the device is set up for MPS\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "# Print feature shapes\n",
    "print(\"Text features shape:\", text_features.shape)\n",
    "print(\"Acoustic features shape:\", acoustic_features.shape)\n",
    "\n",
    "# Input and output dimensions\n",
    "input_dim = text_features.shape[-1]  # Assuming both text and audio features have the same dimension\n",
    "output_dim = 768  # Desired output dimension\n",
    "\n",
    "# Initialize the model and move it to the appropriate device\n",
    "model = CrossModalAttention(input_dim=input_dim, output_dim=output_dim).to(device)\n",
    "\n",
    "# Move features to the same device\n",
    "text_features = text_features.to(device)\n",
    "acoustic_features = acoustic_features.to(device)\n",
    "\n",
    "# Forward pass through the model\n",
    "F1 = model(text_features, acoustic_features)\n",
    "\n",
    "# Print the shape of the output\n",
    "print(F1.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
